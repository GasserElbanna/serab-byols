# BYOL-S Audio Representaion

A data-driven audio embedding consisting of a pretrained [BYOL-S](https://arxiv.org/abs/2110.03414) model with speech samples. Serves as our submission for the [HEAR 2021 competition](https://neuralaudio.ai/hear2021-holistic-evaluation-of-audio-representations.html) and is implemented based on the [common API](https://neuralaudio.ai/hear2021-holistic-evaluation-of-audio-representations.html#common-api) required by the competition evaluation.


### Installation

Tested with Python 3.8 and 3.7.

**Method: pip local source tree**

```python
git clone https://github.com/GasserElbanna/serab-byols.git
python3 -m pip install -e ./serab-byols
```

### BYOL-S Model

The BYOL-S model inputs log-scaled Mel-frequency spectrograms using a
64-band Mel filter. Each frame of the spectrogram is then projected to 2048
dimensions using pretrained encoder. Weights for the projection matrix were
generated by training the BYOL-A network and are stored in this repository in the
file `checkpoints/default2048_BYOLAs64x96-2105311814-e100-bs256-lr0003-rs42.pth`.

### Encoders:

The BYOL-S model has been trained with different encoder architectures:
* AudioNTT: Original encoder used in [BYOL-A](https://arxiv.org/abs/2103.06695)
* Resnetish34: Adapted from this [repo](https://github.com/daisukelab/sound-clf-pytorch/blob/master/src/models.py)
* CLSTM: Inspired from this [paper](https://www.degruyter.com/document/doi/10.1515/jisys-2018-0372/html?lang=de#j_jisys-2018-0372_ref_030)
* CvT: Adapted from this [repo](https://github.com/lucidrains/vit-pytorch#cvt)


### Usage

Audio embeddings can be computed using one of two methods: 1)
`get_scene_embeddings`, or 2) `get_timestamp_embeddings`.

`get_scene_embeddings` accepts a batch of audio clips and produces a single embedding
for each audio clip. This can be computed like so:
```python
import torch
import serab_byols

model_name = 'default'
# Load model with weights - located in the root directory of this repo
model = serab_byols.load_model("checkpoints/default2048_BYOLAs64x96-2105311814-e100-bs256-lr0003-rs42.pth", model_name)

# Create a batch of 2 white noise clips that are 2-seconds long
# and compute scene embeddings for each clip
audio = torch.rand((2, model.sample_rate * 2))
embeddings = serab_byols.get_scene_embeddings(audio, model)
```

The `get_timestamp_embeddings` method works exactly the same but returns an array
of embeddings from audio segment computed every 50ms (could be changed) over the duration of the input audio. An array
of timestamps corresponding to each embedding is also returned.

```python
import torch
import serab_byols

model_name = 'cvt'
# Load model with weights - located in the root directory of this repo
model = serab_byols.load_model("checkpoints/cvt_s1-d1-e64_s2-d1-e256_s3-d1-e512_BYOLAs64x96-osandbyolaloss6373-e100-bs256-lr0003-rs42.pth", model_name)

# Create a batch of 2 white noise clips that are 2-seconds long
# and compute scene embeddings for each clip
frame_duration = 1000 #ms
hop_size = 50 #ms
audio = torch.rand((2, model.sample_rate * 2))
embeddings, timestamps = serab_byols.get_timestamp_embeddings(audio, model, frame_duration, hop_size)
```
