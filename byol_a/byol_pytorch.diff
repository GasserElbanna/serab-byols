--- byol_pytorch.py	2021-04-22 18:42:17.388107173 +0900
+++ byol_a/byol_pytorch.py	2021-04-22 11:03:32.925792995 +0900
@@ -1,3 +1,12 @@
+"""BYOL for Audio
+
+Kudos to Phil Wang, this implementation is based on https://github.com/lucidrains/byol-pytorch/
+
+This code is customized to enable:
+- Decoupling augmentations.
+- Feeding two augmented input batches independently.
+"""
+
 import copy
 import random
 from functools import wraps
@@ -6,7 +15,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 
-from torchvision import transforms as T
+import numpy as np
+
 
 # helper functions
 
@@ -44,18 +54,6 @@ def loss_fn(x, y):
     y = F.normalize(y, dim=-1, p=2)
     return 2 - 2 * (x * y).sum(dim=-1)
 
-# augmentation utils
-
-class RandomApply(nn.Module):
-    def __init__(self, fn, p):
-        super().__init__()
-        self.fn = fn
-        self.p = p
-    def forward(self, x):
-        if random.random() > self.p:
-            return x
-        return self.fn(x)
-
 # exponential moving average
 
 class EMA():
@@ -157,44 +155,27 @@ class NetWrapper(nn.Module):
 
 # main class
 
+
 class BYOL(nn.Module):
+    """BYOL training module that is:
+    - Decoupled augmentations.
+    - Accepts two augmented inputs independently.
+    """
+
     def __init__(
         self,
         net,
         image_size,
-        hidden_layer = -2,
-        projection_size = 256,
-        projection_hidden_size = 4096,
-        augment_fn = None,
-        augment_fn2 = None,
-        moving_average_decay = 0.99,
-        use_momentum = True
+        hidden_layer=-1,
+        projection_size=256,
+        projection_hidden_size=4096,
+        moving_average_decay=0.99,
+        use_momentum=True,
+        channels=1,
     ):
         super().__init__()
         self.net = net
 
-        # default SimCLR augmentation
-
-        DEFAULT_AUG = torch.nn.Sequential(
-            RandomApply(
-                T.ColorJitter(0.8, 0.8, 0.8, 0.2),
-                p = 0.3
-            ),
-            T.RandomGrayscale(p=0.2),
-            T.RandomHorizontalFlip(),
-            RandomApply(
-                T.GaussianBlur((3, 3), (1.0, 2.0)),
-                p = 0.2
-            ),
-            T.RandomResizedCrop((image_size, image_size)),
-            T.Normalize(
-                mean=torch.tensor([0.485, 0.456, 0.406]),
-                std=torch.tensor([0.229, 0.224, 0.225])),
-        )
-
-        self.augment1 = default(augment_fn, DEFAULT_AUG)
-        self.augment2 = default(augment_fn2, self.augment1)
-
         self.online_encoder = NetWrapper(net, projection_size, projection_hidden_size, layer=hidden_layer)
 
         self.use_momentum = use_momentum
@@ -208,7 +189,9 @@ class BYOL(nn.Module):
         self.to(device)
 
         # send a mock image tensor to instantiate singleton parameters
-        self.forward(torch.randn(2, 3, image_size, image_size, device=device))
+        with torch.no_grad():
+            self.forward(torch.randn(2, channels, image_size[0], image_size[1]),
+                         torch.randn(2, channels, image_size[0], image_size[1]))
 
     @singleton('target_encoder')
     def _get_target_encoder(self):
@@ -225,16 +208,12 @@ class BYOL(nn.Module):
         assert self.target_encoder is not None, 'target encoder has not been created yet'
         update_moving_average(self.target_ema_updater, self.target_encoder, self.online_encoder)
 
-    def forward(
-        self,
-        x,
+    def forward(self, image_one, image_two,
         return_embedding = False,
         return_projection = True
     ):
         if return_embedding:
-            return self.online_encoder(x, return_projection = return_projection)
-
-        image_one, image_two = self.augment1(x), self.augment2(x)
+            return self.online_encoder(x, return_projection=return_projection)
 
         online_proj_one, _ = self.online_encoder(image_one)
         online_proj_two, _ = self.online_encoder(image_two)
